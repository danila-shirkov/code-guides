---
title: ¿Qué es el tiempo?
subtitle: O cómo evitar los obstáculos temporales
description: Cómo los ordenadores modernos almacenan información, cómo cuentan los segundos, por qué casi nunca se equivocan y cómo trabajar con el tiempo
image: /assets/images/time/time.png
author: Alexander Verigo
---

## ¿Cómo determina el ordenador el tiempo humano?

Imaginemos la siguiente situación: has encontrado un viejo ordenador en el trastero y has decidido encenderlo. El ordenador se enciende, a pesar de haber estado inactivo durante 10 años, pero hay un problema: el sistema cree que hoy es 1 de enero de 1970. Esta situación nos ayudará a entender cómo los ordenadores almacenan la hora actual: después de todo, no tienen resortes ni péndulos en su interior, como los relojes, que podrían contar los segundos. ¿Cómo muestran la hora correcta si se apagan durante varias horas y por qué el mismo principio no funciona si el ordenador ha estado en el trastero durante 10 años?

### El péndulo como el primer cronómetro

Antes de responder a estas preguntas, vamos a entender cómo se mide el tiempo con precisión. No es una tarea tan sencilla si no tienes un reloj a mano. Podemos contar los días, los meses y los años o dividir el día en horas aproximadas. Pero encontrar un punto de referencia que indique la cantidad de segundos transcurridos es mucho más difícil.

Se considera que el primer científico que resolvió este problema fue el científico italiano [Galileo Galilei](https://es.wikipedia.org/wiki/Galileo_Galilei) en el siglo XVII. Galileo necesitaba saber con precisión cuánto duraba un determinado experimento. Sus biógrafos afirman que al principio contaba sus propios latidos del corazón, pero una vez observó un candelabro oscilante en la catedral de Pisa y se dio cuenta de que cada oscilación del candelabro duraba la misma cantidad de latidos del corazón. Este número no cambia cuando la amplitud de los movimientos disminuye. Así que se dio cuenta de que las oscilaciones del péndulo ayudan a medir el tiempo con precisión.

### ¿Se pueden utilizar las oscilaciones para medir el tiempo en un ordenador?

Cuando en los años 70 y 80 los ingenieros se enfrentaron por primera vez al problema de mostrar la hora humana en un ordenador, curiosamente recurrieron a los trabajos del científico italiano, pero los miraron desde un ángulo diferente. El péndulo no es la única fuente posible de oscilaciones. Cualquier ordenador personal moderno tiene un "corazón": la unidad central de procesamiento (CPU). Es un dispositivo muy complejo que realiza muchas funciones, pero en este caso nos interesa una de sus partes. Cuando pasa corriente a través de él, emite oscilaciones eléctricas uniformes con una [frecuencia](https://es.wikipedia.org/wiki/Frecuencia) constante. La frecuencia de estas oscilaciones se mide en hercios (Hz) y determina la cantidad de operaciones que la CPU puede realizar en un segundo. Desafortunadamente, si utilizamos la única operación disponible para nosotros en un segundo para aumentar el contador de oscilaciones, nuestra CPU será completamente inútil para cualquier otra cosa: no tendrá tiempo para ejecutar otras instrucciones. Por supuesto, las CPU modernas no son tan lentas, su frecuencia se mide en gigahercios (GHz), es decir, en 1 000 000 000 operaciones por segundo. Pero incluso con tal potencia, tendríamos que contar cada oscilación de la CPU para medir el tiempo. Para resolver este problema, se inventó el generador de cuarzo. Es una fina placa de silicio que se expande y contrae uniformemente bajo la influencia de la corriente eléctrica, generando una débil carga eléctrica en su superficie [efecto piezoeléctrico](https://es.wikipedia.org/wiki/Efecto_piezoel%C3%A9ctrico). Por lo tanto, las oscilaciones mecánicas de la placa van acompañadas de oscilaciones eléctricas sincrónicas y uniformes.

![oscillator](/assets/images/time/oscillator.png)

Las oscilaciones uniformes resultantes se llaman temporizador, ya que permiten a la CPU sincronizar las operaciones que realiza en el tiempo. Por lo tanto, el ordenador tiene su propia comprensión del tiempo.

### Del tiempo del ordenador al tiempo humano

Resulta que el ordenador tiene un temporizador interno, pero no mide el tiempo humano, que es bastante inútil para él. Por lo general, esta tarea la asumen los sistemas operativos (SO). Dado que el SO sabe a qué frecuencia funciona el generador de cuarzo, puede medir el tiempo que pasa entre las activaciones del temporizador (la activación del temporizador se llama tic o jiffy). Si el generador funciona a una frecuencia de 100 Hz, entonces el período entre los tics es de 1/100 de segundo o 10 milisegundos. El sistema operativo crea una variable en la memoria, que generalmente se llama jiffies, y la incrementa en uno cada vez que la CPU envía una señal de tic.

Por lo tanto, para saber cuánto tiempo ha estado encendido el ordenador, el sistema sólo tiene que multiplicar el tamaño del período entre los tics por el número de tics. Y para saber la hora actual, sólo tiene que añadir el tiempo transcurrido al tiempo en el momento del inicio del sistema. Pero, ¿cómo se puede saber la hora humana en el momento del inicio del sistema?

### RTC o reloj en tiempo real

Los primeros ordenadores personales (por ejemplo, [IBM PC](https://es.wikipedia.org/wiki/IBM_Personal_Computer#Design_process) o [Apple II](https://es.wikipedia.org/wiki/Apple_II)) no podían seguir el tiempo transcurrido después de apagarse y [preguntaban por él al encenderse](https://www.youtube.com/watch?v=X3aqJQPQKhs). Para resolver este problema, el generador de cuarzo resultó ser útil de nuevo. No importa de qué fuente se obtenga la corriente eléctrica para este esquema, lo que llevó a los desarrolladores a la idea de que era suficiente conectar el generador a una simple batería de litio para obtener oscilaciones eléctricas uniformes.

Si añadimos un contador binario a este esquema, que se incrementa con cada oscilación, obtendremos un dispositivo que puede registrar el tiempo humano. Se llama RTC (Real Time Clock), o reloj en tiempo real. La frecuencia de las oscilaciones en la salida del RTC suele ser de 32 768 Hz o 2^15 Hz, lo que es conveniente para utilizar en contadores binarios.

![RTC](/assets/images/time/RTC.png)

El viejo ordenador del que hablamos al principio confundió la hora precisamente porque la batería de su reloj en tiempo real se agotó: después de encenderlo, no pudo leer la hora del RTC y estableció la hora de inicio estándar - 1 de enero de 1970.

### NTP o protocolo de tiempo de red

El RTC permite al ordenador contar los milisegundos, incluso cuando está apagado. Pero los RTC estándar tienen un error de 1,7-8,6 segundos al día, lo que significa que pueden perder una hora completa en un año.

A veces nos encontramos con este problema en la vida real: por ejemplo, cuando tenemos que ajustar manualmente la hora en un reloj de cuarzo de pulsera o en un microondas, que también utiliza un RTC. Pero nunca tenemos que hacerlo en un ordenador. Los ordenadores modernos ajustan la hora a través de Internet utilizando el NTP (Network Time Protocol). Este protocolo tiene en cuenta incluso el tiempo de transmisión de los datos entre la fuente y el ordenador y lo compensa. En una red pública, el error es de sólo 10 ms.

### ¿Cómo funcionan juntos los protocolos?

![computer timekeeping](/assets/images/time/computer_time_keeping_system.png)

Cuando nuestro ordenador está apagado, el RTC sigue funcionando y cuenta el tiempo. Cuando pulsamos el botón de encendido, el sistema operativo recoge la hora del RTC y comienza a rastrear el tiempo por sí mismo utilizando el temporizador de la CPU. De vez en cuando, el sistema operativo recibe la hora exacta a través del NTP y ajusta su contador interno. Cuando apagamos el ordenador, el RTC se hace cargo de nuevo.

## Marcas de tiempo y épocas

Como se mencionó anteriormente, para rastrear el tiempo humano, el sistema operativo crea una variable en la memoria del ordenador llamada jiffy, que almacena el número de tics desde el inicio del sistema. Pero, ¿cómo se puede mostrar un calendario al usuario utilizando esta variable?

### Marca de tiempo de Unix y época

En los años 70, los ingenieros de Bell Labs resolvieron este problema al desarrollar el sistema operativo Unix (que sentó las bases de los modernos sistemas Linux y MacOS). Introdujeron una variable en el sistema que, a partir de una fecha específica, se incrementa con cada tic del generador, y la llamaron época.

Se asigna un número entero con signo (entero con signo) de 32 bits a esta variable (es decir, desde -2 147 483 648 (-2^31) hasta 2 147 483 647 (2^31−1)). La mayoría de los generadores en ese momento funcionaban a una frecuencia de 60 Hz, es decir, contaban 60 tics por segundo, por lo que la variable almacenaba 1/60 de segundo y podía representar un intervalo de tiempo de no más de 829 días.

En la [versión de Unix de 1971](http://man.cat-v.org/unix-1st/2/sys-time), el recuento comenzó el 1 de enero de 1971. [Al año siguiente](https://minnie.tuhs.org/cgi-bin/utree.pl?file=V3/man/man2/time.2), el 1 de enero de 1972. Fue en la [cuarta versión de Unix](https://minnie.tuhs.org/cgi-bin/utree.pl?file=V4/man/man2/time.2) en 1973 cuando se tomó la fecha del 1 de enero de 1970 como época, y la variable comenzó a almacenar no 1/60 de segundo, sino un segundo completo. Más tarde, este principio se convirtió en un estándar internacional y se utiliza hasta el día de hoy.

Si tienes un ordenador con un sistema operativo de la familia Linux o MacOS, puedes ver la marca de tiempo de Unix actual escribiendo en la terminal:

```shell
date +"%s"
```

Windows también cuenta el tiempo a partir de una fecha específica, pero utiliza el año 1601 en lugar del año 1970 y almacena la hora en un formato diferente.

### El problema del año 2038

Dado que el tiempo se almacena en una variable entera de 32 bits y representa la cantidad de segundos transcurridos desde la época de Unix, el mayor número de segundos que podemos utilizar es 2 147 483 647 (2^31−1). Si añadimos esta cantidad a la época, obtendremos el 19 de enero de 2038 a las 03:14. ¿Qué pasará con el sistema cuando llegue ese día y pase otro segundo?

Para responder a esta pregunta, consideremos una característica de las variables de 32 bits:

2 147 483 647 en sistema binario se representa como

```js
01111111111111111111111111111111 // (1 cero y 31 unos)
```

Si aumentamos este número en uno, se convertirá en

```js
10000000000000000000000000000000 // (1 uno y 31 ceros)
```

La primera cifra en tales números indica el signo: `+` o `-`. Y esto significa que en el sistema decimal será igual a -2 147 483 648, es decir, el número pasará del más grande positivo al más pequeño negativo. Y el sistema mostrará la fecha que es la diferencia entre la época y dos mil millones de segundos, es decir, el 13 de diciembre de 1901. Pero no hay de qué preocuparse, el problema ya está resuelto: la mayoría de los sistemas utilizan números de 64 bits para almacenar el tiempo. Esto es suficiente para evitar el problema hasta las 15:40 del 4 de diciembre de 292 277 026 596.

### El problema del año 2000 o el error del milenio

> "640 kilobytes ought to be enough for anybody." - Bill Gates.

En realidad, ya hemos tenido una situación similar en la historia. En los años 60 y 70 del siglo pasado, cuando la gente empezó a escribir software para ordenadores, las placas de memoria eran caras y la mayoría de los ordenadores tenían sólo unos pocos kilobytes. Para ahorrar memoria, los programadores decidieron escribir las fechas en formato DD.MM.AA.

¿Qué problema causó esto? Supongamos que tenemos una persona llamada Bob, cuya fecha de nacimiento se registra como 01.11.19. Nació en 1919 y tiene más de 100 años. Hay una persona llamada Fred y nació el 02.11.19, pero sólo tiene dos años porque nació en 2019. Al encontrarse con este formato de fechas, una persona puede entender el contexto, pero un ordenador no puede hacerlo.

Los programadores que escribieron programas en los años 60 y 70 ni siquiera se imaginaban que su código podría sobrevivir hasta el año 2000, por lo que el uso de dos dígitos en lugar de cuatro era una optimización normal. Pero cuando se acercaba el año 2000 y muchas empresas todavía utilizaban el mismo formato de fechas, comenzó el pánico: nadie sabía qué pasaría cuando el año 99 cambiara a 00. En ese momento, se difundieron todo tipo de rumores en el mundo: por ejemplo, que los cajeros automáticos comenzarían a escupir dinero en ese momento, o que los aviones comenzarían a caer del cielo. El error fue detectado a tiempo y se logró solucionar en la mayoría de los sistemas a tiempo.

## Conclusiones

- Cualquier ordenador tiene un procesador que puede generar oscilaciones a intervalos regulares.
- Conociendo el período de estas oscilaciones, el sistema operativo del ordenador puede contar el tiempo humano.
- A través del protocolo NTP, el sistema operativo puede obtener la hora exacta de los servidores de tiempo para ajustar sus relojes.
- Cuando el ordenador está apagado, el RTC sigue funcionando y cuenta el tiempo.
- Epoch es una fecha a partir de la cual se cuenta el tiempo hacia adelante o hacia atrás. Diferentes sistemas operativos y lenguajes de programación pueden elegir su propio epoch.
- Unix-timestamp es la cantidad de segundos transcurridos desde el epoch de Unix, es decir, desde las 00:00 del 1 de enero de 1970.
- Nunca escribas código que describa el tiempo humano por ti mismo.
- Si necesitas una secuencia precisa de eventos en el tiempo, no utilices zonas horarias. Almacena el tiempo en UTC y conviértelo según sea necesario.

## Recursos adicionales

- [Cómo los ordenadores suman números](https://www.youtube.com/watch?v=YuSgZ173Utg)